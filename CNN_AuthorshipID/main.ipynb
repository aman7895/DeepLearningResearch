{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import json\n",
    "import py_crepe\n",
    "import datetime\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "import fixeddocdata\n",
    "import string\n",
    "import pandas as pd\n",
    "np.random.seed(0123)  # for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "\n",
    "subset = None\n",
    "\n",
    "#Whether to save model parameters\n",
    "save = False\n",
    "model_name_path = 'params/crepe_model.json'\n",
    "model_weights_path = 'params/crepe_model_weights.h5'\n",
    "\n",
    "#Maximum length. Longer gets chopped. Shorter gets padded.\n",
    "maxlen = 1014\n",
    "\n",
    "#Model params\n",
    "#Filters for conv layers\n",
    "nb_filter = 128 #initially 256\n",
    "#Number of units in the dense layer\n",
    "dense_outputs = 512 #Initially 1024\n",
    "#Conv layer kernel size\n",
    "filter_kernels = [3, 3, 3, 3, 3, 3]\n",
    "#Number of units in the final output layer. Number of classes.\n",
    "\n",
    "#Compile/fit params\n",
    "batch_size = 32\n",
    "nb_epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "SELECT author_id, doc_content FROM aman_content WHERE author_id IN (80, 53, 1680, 1097, 1103, 114, 132, 176, 1472, 1416, 55, 104) AND doc_id NOT IN (3270, 3489, 12599, 2549)  AND doc_id IN (74, 279, 12641, 12596, 23260, 3395, 2746, 3277, 117, 3270, 2034, 20055, 218, 6979, 3405, 3489, 2537, 281, 21567, 12599, 745, 2549, 2548, 3755) ;\n",
      "Execution completed\n",
      "Read completed\n",
      "Number of rows: 20\n",
      "author_id       int64\n",
      "doc_content    object\n",
      "dtype: object\n",
      "Data Frame created: Shape: (9232, 2)\n",
      "Author:    53  Size:  1079\n",
      "Author:    80  Size:   820\n",
      "Author:   132  Size:  1329\n",
      "Author:   176  Size:   867\n",
      "Author:   114  Size:  1131\n",
      "Author:  1097  Size:   659\n",
      "Author:  1103  Size:   942\n",
      "Author:   104  Size:   264\n",
      "Author:  1472  Size:   579\n",
      "Author:  1416  Size:  1051\n",
      "Author:    55  Size:   147\n",
      "Author:  1680  Size:   364\n",
      "Min: 147\n",
      "Max: 1329\n",
      "Authors [53, 80, 132, 176, 114, 1097, 1103, 104, 1472, 1416, 55, 1680].\n",
      "Found 1764 texts.\n",
      "Found 1764 labels.\n",
      "Creating vocab...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "#Expect x to be a list of sentences. Y to be a one-hot encoding of the\n",
    "#categories.\n",
    "\n",
    "### 515-1122-122 and 1573 with remove 6 layers\n",
    "#authorlist=[121, 479 , 649 ]\n",
    "#doc_id = 14706\n",
    "\n",
    "authorlist=[ 80, 53, 1680, 1097, 1103, 114, 132, 176, 1472, 1416, 55, 104]\n",
    "doc_id = [3270, 3489, 12599, 2549]\n",
    "tdoc = [74, 279, 12641, 12596, 23260, 3395, 2746, 3277, 117, 3270, 2034, 20055, 218, 6979, 3405, 3489, 2537, 281, 21567, 12599, 745, 2549, 2548, 3755]\n",
    "cat_output = len(authorlist) #binary in the last layer\n",
    "\n",
    "# def main(authorlist, doc_id):\n",
    "    \n",
    "    \n",
    "((trainX, trainY), (valX, valY)) = data_helpers.load_ag_data(authors = authorlist, docID = doc_id, tdoc = tdoc)\n",
    "\n",
    "print('Creating vocab...')\n",
    "vocab, reverse_vocab, vocab_size, check = data_helpers.create_vocab_set()\n",
    "\n",
    "\n",
    "#trainX = data_helpers.encode_data(trainX, maxlen, vocab, vocab_size, check)\n",
    "#test_data = data_helpers.encode_data(valX, maxlen, vocab, vocab_size, check)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "classes = len(authorlist)\n",
    "(model, sgd, model_weights_path) = py_crepe.build_model(classes, filter_kernels,\n",
    "                                                        dense_outputs, maxlen, vocab_size, nb_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model...\n",
      "Epoch: 0\n",
      "Epoch 0. Loss: 2.48112207651. Accuracy: 0.0833333333333\n",
      "Epoch time: 0:00:24.563080. Total time: 0:00:24.608400\n",
      "\n",
      "Epoch: 1\n",
      "Epoch 1. Loss: 2.46972413858. Accuracy: 0.140625\n",
      "Epoch time: 0:00:21.090064. Total time: 0:00:45.741279\n",
      "\n",
      "Epoch: 2\n",
      "Epoch 2. Loss: 2.36108263334. Accuracy: 0.166666666667\n",
      "Epoch time: 0:00:21.196528. Total time: 0:01:06.981930\n",
      "\n",
      "Epoch: 3\n",
      "Epoch 3. Loss: 1.96348934372. Accuracy: 0.419270833333\n",
      "Epoch time: 0:00:21.577465. Total time: 0:01:28.601956\n",
      "\n",
      "Epoch: 4\n",
      "Epoch 4. Loss: 1.7992956837. Accuracy: 0.3515625\n",
      "Epoch time: 0:00:21.095828. Total time: 0:01:49.742703\n",
      "\n",
      "Epoch: 5\n",
      "Epoch 5. Loss: 2.06305861473. Accuracy: 0.317708333333\n",
      "Epoch time: 0:00:21.153123. Total time: 0:02:10.939075\n",
      "\n",
      "Epoch: 6\n",
      "Epoch 6. Loss: 1.73783878485. Accuracy: 0.356770833333\n",
      "Epoch time: 0:00:21.271865. Total time: 0:02:32.265066\n",
      "\n",
      "Epoch: 7\n",
      "Epoch 7. Loss: 1.73823209604. Accuracy: 0.442708333333\n",
      "Epoch time: 0:00:21.130239. Total time: 0:02:53.438944\n",
      "\n",
      "Epoch: 8\n",
      "Epoch 8. Loss: 2.16135884511. Accuracy: 0.416666666667\n",
      "Epoch time: 0:00:21.045358. Total time: 0:03:14.529952\n",
      "\n",
      "Epoch: 9\n",
      "Epoch 9. Loss: 3.59328148762. Accuracy: 0.3046875\n",
      "Epoch time: 0:00:21.222873. Total time: 0:03:35.798807\n",
      "\n",
      "Epoch: 10\n",
      "Epoch 10. Loss: 1.88585103303. Accuracy: 0.479166666667\n",
      "Epoch time: 0:00:21.517154. Total time: 0:03:57.361824\n",
      "\n",
      "Epoch: 11\n",
      "Epoch 11. Loss: 2.23954067628. Accuracy: 0.3671875\n",
      "Epoch time: 0:00:21.221009. Total time: 0:04:18.624731\n",
      "\n",
      "Epoch: 12\n",
      "Epoch 12. Loss: 2.05391226604. Accuracy: 0.447916666667\n",
      "Epoch time: 0:00:21.313946. Total time: 0:04:39.982306\n",
      "\n",
      "Epoch: 13\n",
      "Epoch 13. Loss: 2.44646836321. Accuracy: 0.3671875\n",
      "Epoch time: 0:00:21.333961. Total time: 0:05:01.357676\n",
      "\n",
      "Epoch: 14\n",
      "Epoch 14. Loss: 3.14972952008. Accuracy: 0.372395833333\n",
      "Epoch time: 0:00:21.378119. Total time: 0:05:22.786507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fit model...')\n",
    "initial = datetime.datetime.now()\n",
    "for e in xrange(nb_epoch):\n",
    "    xi, yi = data_helpers.shuffle_matrix(trainX, trainY)\n",
    "    xi_test, yi_test = data_helpers.shuffle_matrix(valX, valY)\n",
    "    if subset:\n",
    "        batches = data_helpers.mini_batch_generator(xi[:subset], yi[:subset],\n",
    "                                                    vocab, vocab_size, check,\n",
    "                                                    maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "    else:\n",
    "        batches = data_helpers.mini_batch_generator(xi, yi, vocab, vocab_size,\n",
    "                                                    check, maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    test_batches = data_helpers.mini_batch_generator(xi_test, yi_test, vocab,\n",
    "                                                     vocab_size, check, maxlen,\n",
    "                                                     batch_size=batch_size)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    loss = 0.0\n",
    "    step = 1\n",
    "    start = datetime.datetime.now()\n",
    "    print('Epoch: {}'.format(e))\n",
    "    for x_train, y_train in batches:\n",
    "        \n",
    "        f = model.train_on_batch(x_train, y_train)\n",
    "        loss += f[0]\n",
    "        loss_avg = loss / step\n",
    "        accuracy += f[1]\n",
    "        accuracy_avg = accuracy / step\n",
    "        if step % 100 == 0:\n",
    "            print('  Step: {}'.format(step))\n",
    "            print('\\tLoss: {}. Accuracy: {}'.format(loss_avg, accuracy_avg))\n",
    "        step += 1\n",
    "\n",
    "    test_accuracy = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_step = 1\n",
    "    \n",
    "    for x_test_batch, y_test_batch in test_batches:\n",
    "        f_ev = model.test_on_batch(x_test_batch, y_test_batch)\n",
    "        test_loss += f_ev[0]\n",
    "        test_loss_avg = test_loss / test_step\n",
    "        test_accuracy += f_ev[1]\n",
    "        test_accuracy_avg = test_accuracy / test_step\n",
    "        test_step += 1\n",
    "    stop = datetime.datetime.now()\n",
    "    e_elap = stop - start\n",
    "    t_elap = stop - initial\n",
    "    print('Epoch {}. Loss: {}. Accuracy: {}\\nEpoch time: {}. Total time: {}\\n'.format(e, test_loss_avg, test_accuracy_avg, e_elap, t_elap))\n",
    "\n",
    "if save:\n",
    "    print('Saving model params...')\n",
    "    json_string = model.to_json()\n",
    "    with open(model_name_path, 'w') as f:\n",
    "        json.dump(json_string, f)\n",
    "\n",
    "model.save_weights(model_weights_path)\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('sgd.pickle', 'wb') as handle:\n",
    "    pickle.dump(sgd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del trainX, trainY, valX, valY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_weights_path)\n",
    "\n",
    "#from keras.optimizers import SGD\n",
    "#sgd = SGD(lr=0.01, momentum=0.9, nesterov= True)\n",
    "\n",
    "# Compile model again (required to make predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictModel2(model, textX):\n",
    "    \n",
    "    predY = np.array(model.predict(testX))\n",
    "\n",
    "    predYList = predY[:]\n",
    "    \n",
    "    predY = np.mean(predYList, axis=0)\n",
    "    return (predYList, predY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictModel(model, testX):\n",
    "    # Function to take input of data and return prediction model\n",
    "    predY = np.array(model.predict(testX))\n",
    "\n",
    "    predYList = predY[:]\n",
    "    entro = []\n",
    "    \n",
    "    flag = False\n",
    "    import math\n",
    "    for row in predY:\n",
    "        entroval = 0\n",
    "        for i in row:\n",
    "            if(i <= 0):\n",
    "                flag = True\n",
    "                pass\n",
    "            else:\n",
    "                entroval += (i * (math.log(i , 2)))\n",
    "        entroval = -1 * entroval\n",
    "        entro.append(entroval)\n",
    "        \n",
    "    if(flag == False):\n",
    "        yx = zip(entro, predY)\n",
    "        yx = sorted(yx, key = lambda t: t[0])\n",
    "        newPredY = [x for y, x in yx]\n",
    "        predYEntroList = newPredY[:int(len(newPredY)*0.3)] # Reduce this \n",
    "        predY = np.mean(predYEntroList, axis=0)\n",
    "    else:\n",
    "        predY = np.mean(predYList, axis=0)\n",
    "    \n",
    "    return (predYList, predY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_binary = []\n",
    "test_binary2 = []\n",
    "for docs in doc_id:\n",
    "    (testX, testY) = data_helpers.load_doc_data(authors = authorlist, docID = docs)\n",
    "    testX = data_helpers.encode_data(testX, maxlen, vocab, vocab_size, check)\n",
    "    \n",
    "    (predYListDoc, predYDoc) = predictModel(model, testX)\n",
    "    (predYListFrag, predYFrag) = predictModel2(model, testX)\n",
    "    \n",
    "    print(predYListFrag)\n",
    "    \n",
    "    testY = np.array(testY)\n",
    "    testY = testY.mean(axis = 0)\n",
    "    \n",
    "    predLocationDoc = predYDoc.tolist().index(max(predYDoc))\n",
    "    \n",
    "    if predLocationDoc == testY:\n",
    "        test_binary.append(1)\n",
    "    else:\n",
    "        test_binary.append(0)\n",
    "        \n",
    "    predLocationFrag = predYFrag.tolist().index(max(predYFrag))\n",
    "    \n",
    "    if predLocationFrag == testY:\n",
    "        test_binary2.append(1)\n",
    "    else:\n",
    "        test_binary2.append(0)\n",
    "        \n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predY = np.array(model.predict(testX, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predY.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_binary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfeature_model = py_crepe.build_feature_model()\\nfeature_trainX = feature_model.predict(trainX)\\nfeature_testX = feature_model.predict(testX)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "feature_model = py_crepe.build_feature_model()\n",
    "feature_trainX = feature_model.predict(trainX)\n",
    "feature_testX = feature_model.predict(testX)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
