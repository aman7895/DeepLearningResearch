{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import json\n",
    "import py_crepe\n",
    "import datetime\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "import data\n",
    "import string\n",
    "import pandas as pd\n",
    "import autoEncoder\n",
    "np.random.seed(0123)  # for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "\n",
    "subset = None\n",
    "\n",
    "#Whether to save model parameters\n",
    "save = False\n",
    "model_name_path = 'params/crepe_model.json'\n",
    "model_weights_path = 'params/auto_model_weights.h5'\n",
    "\n",
    "#Maximum length. Longer gets chopped. Shorter gets padded.\n",
    "maxlen = 1014\n",
    "\n",
    "#Model params\n",
    "#Filters for conv layers\n",
    "nb_filter = 128 #initially 256\n",
    "#Number of units in the dense layer\n",
    "dense_outputs = 512 #Initially 1024\n",
    "#Conv layer kernel size\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n",
    "#Number of units in the final output layer. Number of classes.\n",
    "\n",
    "#Compile/fit params\n",
    "batch_size = 32\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "SELECT author_id, doc_content FROM aman_content WHERE author_id IN (3816, 1416, 1472) AND doc_id NOT IN (3756, 3253, 3399, 3393, 13442, 14907) ;\n",
      "Execution completed\n",
      "Read completed\n",
      "Number of rows: 501\n",
      "author_id       int64\n",
      "doc_content    object\n",
      "dtype: object\n",
      "Data Frame created: Shape: (98052, 2)\n",
      "Author:  1416  Size: 33336\n",
      "Author:  1472  Size:  4936\n",
      "Author:  3816  Size: 59780\n",
      "Min: 4936\n",
      "Max: 59780\n",
      "Authors [1416, 1472, 3816].\n",
      "Found 14808 texts.\n",
      "Found 14808 labels.\n",
      "Creating vocab...\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "#Expect x to be a list of sentences. Y to be a one-hot encoding of the\n",
    "#categories.\n",
    "\n",
    "### 515-1122-122 and 1573 with remove 6 layers\n",
    "#authorlist=[121, 479 , 649 ]\n",
    "#doc_id = 14706\n",
    "\n",
    "authorlist=[3816, 1416, 1472]\n",
    "doc_id = [3756, 3253, 3399, 3393, 13442, 14907]\n",
    "cat_output = len(authorlist) #binary in the last layer\n",
    "\n",
    "# def main(authorlist, doc_id):\n",
    "    \n",
    "    \n",
    "((trainX, trainY), (valX, valY)) = data_helpers.load_ag_data(authors = authorlist, docID = doc_id)\n",
    "\n",
    "print('Creating vocab...')\n",
    "vocab, reverse_vocab, vocab_size, check = data_helpers.create_vocab_set()\n",
    "\n",
    "\n",
    "#trainX = data_helpers.encode_data(trainX, maxlen, vocab, vocab_size, check)\n",
    "#test_data = data_helpers.encode_data(valX, maxlen, vocab, vocab_size, check)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "classes = len(authorlist)\n",
    "\n",
    "#(model, model_weights_path) = autoEncoder.auto_model(maxlen, vocab_size)\n",
    "(model, model_weights_path) = autoEncoder.fcc_auto_model(maxlen, vocab_size,\n",
    "                                                         model_weights_path,\n",
    "                                                         dense_outputs, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model...\n",
      "Epoch: 0\n",
      "  Step: 100\n",
      "\tLoss: 1.18011718512. Accuracy: 0.3278125\n",
      "  Step: 200\n",
      "\tLoss: 1.13938054204. Accuracy: 0.32953125\n",
      "  Step: 300\n",
      "\tLoss: 1.12579505285. Accuracy: 0.328541666667\n",
      "Epoch 0. Loss: 1.09871772797. Accuracy: 0.318436380035\n",
      "Epoch time: 0:01:05.422985. Total time: 0:01:05.727957\n",
      "\n",
      "Epoch: 1\n",
      "  Step: 100\n",
      "\tLoss: 1.09866985798. Accuracy: 0.3296875\n",
      "  Step: 200\n",
      "\tLoss: 1.09866061866. Accuracy: 0.32765625\n",
      "  Step: 300\n",
      "\tLoss: 1.09863682389. Accuracy: 0.3309375\n",
      "Epoch 1. Loss: 1.09880826678. Accuracy: 0.318959080083\n",
      "Epoch time: 0:01:03.069619. Total time: 0:02:09.105622\n",
      "\n",
      "Epoch: 2\n",
      "  Step: 100\n",
      "\tLoss: 1.09862119555. Accuracy: 0.3365625\n",
      "  Step: 200\n",
      "\tLoss: 1.09865185857. Accuracy: 0.33359375\n",
      "  Step: 300\n",
      "\tLoss: 1.09862994313. Accuracy: 0.335729166667\n",
      "Epoch 2. Loss: 1.09887279105. Accuracy: 0.317913679827\n",
      "Epoch time: 0:01:02.286320. Total time: 0:03:11.693442\n",
      "\n",
      "Epoch: 3\n",
      "  Step: 100\n",
      "\tLoss: 1.09878999114. Accuracy: 0.3215625\n",
      "  Step: 200\n",
      "\tLoss: 1.09871055543. Accuracy: 0.329375\n",
      "  Step: 300\n",
      "\tLoss: 1.0986643525. Accuracy: 0.33375\n",
      "Epoch 3. Loss: 1.09888740381. Accuracy: 0.318697730059\n",
      "Epoch time: 0:01:03.131091. Total time: 0:04:15.126733\n",
      "\n",
      "Epoch: 4\n",
      "  Step: 100\n",
      "\tLoss: 1.09876683235. Accuracy: 0.32625\n",
      "  Step: 200\n",
      "\tLoss: 1.09847580731. Accuracy: 0.344375\n",
      "  Step: 300\n",
      "\tLoss: 1.09860423366. Accuracy: 0.338020833333\n",
      "Epoch 4. Loss: 1.09888121133. Accuracy: 0.318436380035\n",
      "Epoch time: 0:01:06.851322. Total time: 0:05:22.280214\n",
      "\n",
      "Epoch: 5\n",
      "  Step: 100\n",
      "\tLoss: 1.09870001435. Accuracy: 0.3321875\n",
      "  Step: 200\n",
      "\tLoss: 1.09865511894. Accuracy: 0.3328125\n",
      "  Step: 300\n",
      "\tLoss: 1.09867738883. Accuracy: 0.3334375\n",
      "Epoch 5. Loss: 1.09893952518. Accuracy: 0.318959080083\n",
      "Epoch time: 0:01:06.579965. Total time: 0:06:29.169878\n",
      "\n",
      "Epoch: 6\n",
      "  Step: 100\n",
      "\tLoss: 1.0985175097. Accuracy: 0.3415625\n",
      "  Step: 200\n",
      "\tLoss: 1.09851519704. Accuracy: 0.34203125\n",
      "  Step: 300\n",
      "\tLoss: 1.09858815789. Accuracy: 0.338645833333\n",
      "Epoch 6. Loss: 1.09890292409. Accuracy: 0.318436380035\n",
      "Epoch time: 0:01:09.122690. Total time: 0:07:38.613915\n",
      "\n",
      "Epoch: 7\n",
      "  Step: 100\n",
      "\tLoss: 1.09864483476. Accuracy: 0.3359375\n",
      "  Step: 200\n",
      "\tLoss: 1.09869770885. Accuracy: 0.33078125\n",
      "  Step: 300\n",
      "\tLoss: 1.09864854495. Accuracy: 0.3346875\n",
      "Epoch 7. Loss: 1.09893391978. Accuracy: 0.318959080083\n",
      "Epoch time: 0:01:06.366418. Total time: 0:08:45.293281\n",
      "\n",
      "Epoch: 8\n",
      "  Step: 100\n",
      "\tLoss: 1.09881082296. Accuracy: 0.324375\n",
      "  Step: 200\n",
      "\tLoss: 1.09875719845. Accuracy: 0.3240625\n",
      "  Step: 300\n",
      "\tLoss: 1.09868572712. Accuracy: 0.331458333333\n",
      "Epoch 8. Loss: 1.09898581172. Accuracy: 0.318436380035\n",
      "Epoch time: 0:01:07.471907. Total time: 0:09:53.065202\n",
      "\n",
      "Epoch: 9\n",
      "  Step: 100\n",
      "\tLoss: 1.09869114041. Accuracy: 0.3334375\n",
      "  Step: 200\n",
      "\tLoss: 1.09866540968. Accuracy: 0.33453125\n",
      "  Step: 300\n",
      "\tLoss: 1.09867232243. Accuracy: 0.334479166667\n",
      "Epoch 9. Loss: 1.09898082159. Accuracy: 0.317913679827\n",
      "Epoch time: 0:01:07.624300. Total time: 0:11:00.998475\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimport cPickle as pickle\\nwith open('sgd.pickle', 'wb') as handle:\\n    pickle.dump(sgd, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Fit model...')\n",
    "initial = datetime.datetime.now()\n",
    "for e in xrange(nb_epoch):\n",
    "    xi, yi = data_helpers.shuffle_matrix(trainX, trainY)\n",
    "    xi_test, yi_test = data_helpers.shuffle_matrix(valX, valY)\n",
    "    if subset:\n",
    "        batches = data_helpers.mini_batch_generator(xi[:subset], yi[:subset],\n",
    "                                                    vocab, vocab_size, check,\n",
    "                                                    maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "    else:\n",
    "        batches = data_helpers.mini_batch_generator(xi, yi, vocab, vocab_size,\n",
    "                                                    check, maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    test_batches = data_helpers.mini_batch_generator(xi_test, yi_test, vocab,\n",
    "                                                     vocab_size, check, maxlen,\n",
    "                                                     batch_size=batch_size)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    loss = 0.0\n",
    "    step = 1\n",
    "    start = datetime.datetime.now()\n",
    "    print('Epoch: {}'.format(e))\n",
    "    for x_train, y_train in batches:\n",
    "        \n",
    "        f = model.train_on_batch(x_train, y_train)\n",
    "        loss += f[0]\n",
    "        loss_avg = loss / step\n",
    "        accuracy += f[1]\n",
    "        accuracy_avg = accuracy / step\n",
    "        if step % 100 == 0:\n",
    "            print('  Step: {}'.format(step))\n",
    "            print('\\tLoss: {}. Accuracy: {}'.format(loss_avg, accuracy_avg))\n",
    "        step += 1\n",
    "\n",
    "    test_accuracy = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_step = 1\n",
    "    \n",
    "    for x_test_batch, y_test_batch in test_batches:\n",
    "        f_ev = model.test_on_batch(x_test_batch, y_test_batch)\n",
    "        test_loss += f_ev[0]\n",
    "        test_loss_avg = test_loss / test_step\n",
    "        test_accuracy += f_ev[1]\n",
    "        test_accuracy_avg = test_accuracy / test_step\n",
    "        test_step += 1\n",
    "    stop = datetime.datetime.now()\n",
    "    e_elap = stop - start\n",
    "    t_elap = stop - initial\n",
    "    print('Epoch {}. Loss: {}. Accuracy: {}\\nEpoch time: {}. Total time: {}\\n'.format(e, test_loss_avg, test_accuracy_avg, e_elap, t_elap))\n",
    "\n",
    "if save:\n",
    "    print('Saving model params...')\n",
    "    json_string = model.to_json()\n",
    "    with open(model_name_path, 'w') as f:\n",
    "        json.dump(json_string, f)\n",
    "\n",
    "model.save_weights(model_weights_path)\n",
    "\n",
    "'''\n",
    "import cPickle as pickle\n",
    "with open('sgd.pickle', 'wb') as handle:\n",
    "    pickle.dump(sgd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del trainX, trainY, valX, valY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_weights_path)\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.01, momentum=0.9, nesterov= True)\n",
    "\n",
    "# Compile model again (required to make predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_binary = []\n",
    "for docs in doc_id:\n",
    "    (testX, testY) = data_helpers.load_doc_data(authors = authorlist, docID = docs)\n",
    "    testX = data_helpers.encode_data(testX, maxlen, vocab, vocab_size, check)\n",
    "    predY = np.array(model.predict(testX, batch_size=batch_size))\n",
    "    testY = np.array(testY)\n",
    "    predY = predY.mean(axis = 0)\n",
    "    testY = testY.mean(axis = 0)\n",
    "    predLocation = predY.tolist().index(max(predY))\n",
    "    if predLocation == testY:\n",
    "        test_binary.append(1)\n",
    "    else:\n",
    "        test_binary.append(0)\n",
    "    \n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predY = np.array(model.predict(testX, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predY.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfeature_model = py_crepe.build_feature_model()\\nfeature_trainX = feature_model.predict(trainX)\\nfeature_testX = feature_model.predict(testX)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "feature_model = py_crepe.build_feature_model()\n",
    "feature_trainX = feature_model.predict(trainX)\n",
    "feature_testX = feature_model.predict(testX)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
